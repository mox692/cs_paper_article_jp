# [日本語訳] MapReduce: 大規模クラスタにおけるデータ処理の簡素化 -MapReduce: Simplified Data Processing on Large Clusters-
> *Original Article*: https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf
> 
> This article is translated by [@mox692](https://twitter.com/mox692). Please contact me if any problem.

# Abstruct
MapReduceは、大規模なデータセットを処理・生成するためのプログラミングモデルおよびその実装である。ユーザは、キー/値のペアを処理して中間キー/値のペアのセットを生成するmap関数と、同じ中間キーに関連するすべての中間値をマージするreduce関数を指定する。  

この関数型で書かれたプログラムは、自動的に並列化され、コモディティマシンの大規模クラスタ上で実行される。ランタイムシステムは、入力データの分割、マシンの集合に対するプログラムの実行スケジューリング、マシン故障の処理、必要なマシン間通信の管理などの細部を担当する。  

MapReduceの実装は、コモディティマシンの大規模クラスタ上で動作し、高いスケーラビリティを有しています。


# 1 Introduction 
過去5年間、著者やGoogleの他の多くの人々は、クロールされた文書やWebリクエストログなどの大量の生データを処理して、逆指数、Web文書のグラフ構造の様々な表現、ホストごとにクロールされたページ数の要約、特定の日に最も頻繁に行われたクエリのセットなど、様々な種類の派生データを計算する特別目的の計算を何百と実装してきました。このような計算のほとんどは、概念的に簡単です。しかし、入力データは通常大きく、妥当な時間内に計算を終了させるためには、数百、数千のマシンに分散させる必要がある。  

このような複雑さへの対応として、我々は、実行しようとする単純な計算を表現しつつ、並列化、フォールトトレランス、データ分散、負荷分散などの厄介な詳細をライブラリに隠すことができる新しい抽象化を設計しました。この抽象化は、Lispや他の多くの関数型言語に存在するmapとreduceのプリミティブにヒントを得ています。私たちは、ほとんどの計算が、中間的なキーと値のペアのセットを計算するために、入力の各論理的「レコード」にマップ操作を適用し、派生データを適切に結合するために、同じキーを共有するすべての値にリデュース操作を適用していることに気がつきました。  

本研究の主要な貢献は，大規模計算の自動並列化と分散化を可能にするシンプルで強力なインタフェースと，コモディティPCの大規模クラスタ上で高い性能を達成するこのインタフェースの実装にある．第3章では、クラスタベースの計算環境に合わせたMapReduceインタフェースの実装を説明する。セクション4では、私たちが有用と考えたプログラミングモデルのいくつかの改良について説明します。セクション5では、様々なタスクに対する我々の実装の性能測定を行っている。セクション 6 では、Google における MapReduce の使用について、私たちのプロダクションインデックスシステムの書き換えのための基礎として使用した経験を含めて、調査しています。セクション 7 では、関連および将来の仕事について議論します。

# 2 Programming Model
この計算では，入力キー/値ペアのセットを受け取り，出力キー/値ペアのセットを生成する．MapReduceライブラリのユーザは、この計算を2つの関数で表現します。MapとReduceです。 

Map はユーザによって書かれ、入力ペアを受け取り、中間的なキー/値のペアのセットを生成する。MapReduceライブラリは、同じ中間キーIに関連するすべての中間値をグループ化し、Reduce関数に渡します。 

Reduce 関数もユーザーによって書かれ、中間キー I とそのキーに対応する値のセットを受け取ります。Reduce関数は、中間キーIとそのキーに対応する値のセットを受け取り、これらの値をマージして、より小さい値のセットを形成します。通常、1回のReduce呼び出しで生成される出力値は0または1つだけです。中間値は、イテレータを介してユーザーのReduce関数に供給されます。これにより、メモリに収まらないような値のリストを処理することができます。

## 2.1 Example
大規模な文書の集まりの中で、各単語の出現回数を数えるという問題を考えてみよう。ユーザは次のような疑似コードを書くことになる。

```
map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
        EmitIntermediate(w, "1");

reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));
```

さらに、ユーザはmapreducespecificationオブジェクトに、入力ファイルと出力ファイルの名前、およびオプションの調整パラメータを記入するコードを記述します。次に、ユーザは MapReduce 関数を呼び出して、その指定オブジェクトを渡します。ユーザーのコードはMapReduceライブラリ（C++で実装）と一緒にリンクされています。付録Aは、この例のプログラム全文である。

## 2.2 Types
前の擬似コードは文字列の入力と出力で書かれているが、概念的には、ユーザーによって供給されるmapとreduce関数は関連する型を持っている。

```
    map (k1,v1) → list(k2,v2)
    reduce (k2,list(v2)) → list(v2)
```

つまり、入力のキーと値は、出力のキーと値とは異なる領域から引き出される。C++の実装では、ユーザー定義関数との間で文字列の受け渡しを行い、文字列と適切な型との変換はユーザーコードに委ねています。

## 2.3 More Examples
ここでは、MapReduceの計算として簡単に表現できる興味深いプログラムの例をいくつか紹介します。

* 分散Grep: map関数は，与えられたパターンにマッチすれば1行を出力する．reduce関数は、与えられた中間データを出力にコピーするだけのidentity関数である。

* URLアクセス頻度カウント: map関数は、Webページのリクエストのログを処理し。`〈URL, 1〉`を出力する。reduce関数は同じURLの値をすべて足し合わせて `〈URL, total count〉` を出力する。

* Web-Linkの逆グラフ: map関数は `source` という名前のページで見つかった`targetURL` への各リンクに対して `〈target, source〉` というペアを出力する。reduce関数は、与えられたターゲットURLに関連するすべてのソースURLのリストを連結し、 `〈target, list(source)〉` のペアを出力します。

* ホストごとの用語ベクトル: 用語ベクトルは，文書または文書集合に出現する最も重要な単語を `〈word, frequency〉` のペアのリストとして要約したものである．map関数は、各入力文書に対して、ホスト名と用語ベクトルのペア `〈hostname, term vector〉` を生成します（ホスト名は文書のURLから抽出されます）。 reduce関数は与えられたホストに対する文書ごとの全ての用語ベクトルを渡される。reduce関数は、これらの用語ベクトルを足し合わせ、頻度の低い用語を捨て、最終的なホスト名と用語ベクトルのペア `〈hostname, term vector〉` を出力します。

* 転置インデックス: map関数は各文書を解析し，`〈word, documentId〉` のペアを生成します．reduce関数は、与えられたwordのすべてのペアを受け取り、対応する文書IDをソートして、`〈word, list(documentId)〉` のペアを出力します。すべての出力ペアの集合は、単純な転置インデックスを形成する。この計算を拡張して単語の位置を追跡することは容易である。

* 分散ソート: map関数は，各レコードからキーを抽出し， `〈key, record〉` のペアを出力する．reduce関数はすべてのペアを未変更のまま出力する。この計算は，第4.1節で説明したパーティショニング機能と第4.2節で説明した順序付けの特性に依存する．


# 3 Implementation
MapReduceインターフェースの多くの異なる実装が可能である。正しい選択は環境に依存する。例えば、ある実装は小さな共有メモリ・マシンに、別の実装は大きなNUMAマルチプロセッサに、さらに別の実装はネットワーク・マシンのさらに大きな集合に適しているかもしれません。このセクションでは、Googleで広く使われているコンピュータ環境：スイッチ・イーサネットで互いに接続された汎用PCの大きなクラスタ [4] を対象とした実装を説明します。この環境では

(1) マシンは Linux が動作するデュアルプロセッサの x86 プロセッサであり、マシンあたり 2-4GB のメモリを搭載している。

(2) 汎用的なネットワーク・ハードウェアを使用し、マシン・レベルでは100メガビット/秒または1ギガビット/秒、全体では平均してそれ以下の帯域幅を使用します。

(3) クラスタは数百台から数千台のマシンで構成されるため、マシンの故障がよく発生する。

(4) ストレージは各マシンに直接接続された安価な IDE ディスクで提供される．これらのディスクに保存されたデータは、自社開発の分散ファイルシステム[8]で管理されています。このファイルシステムは、信頼性の低いハードウェアの上に可用性と信頼性を提供するために、レプリケーションを使用しています。

(5) ユーザはスケジューリングシステムにジョブを投入する。各ジョブはタスクのセットで構成され、スケジューラによってクラスタ内の利用可能なマシン群にマッピングされます。

## 3.1 Execution Overview
Mapの実行は、入力データを自動的にM個の分割に分割し、複数のマシンに分散させる。入力の分割は異なるマシンで並列に処理することができる。Reduceは中間鍵空間を分割関数（例：hash(key) mod R）を用いてR個に分割することにより分散処理される。図1に、本実装におけるMapReduce処理の全体フローを示す。ユーザプログラムが MapReduce 関数を呼び出すと、以下の一連の動作が発生する（図 1 の番号付きラベルは、以下のリストの番号に対応する）。

1. ユーザープログラム内のMapReduceライブラリは、まず入力ファイルを1個あたり通常16メガバイトから64メガバイト（MB）のM個に分割する（オプションのパラメータでユーザーが制御可能）。その後、クラスタマシン上でプログラムのコピーを多数起動します。

2. プログラムのコピーのうち1つは特別なもの、つまりマスターです。残りはマスターから仕事を割り当てられたワーカーです。M個のマップタスクとR個のリデュースタスクが割り当てられる。マスターはアイドル状態のワーカーを選び、各ワーカーにマップタスクとリデュースタスクを割り当てる。

3. マップタスクを割り当てられたワーカーは、対応する入力スプリットの内容を読み取る。Mapタスクを割り当てられたワーカーは、入力データからキーと値のペアを解析し、それぞれのペアをユーザー定義のMap関数に渡します。Map関数が生成する中間値はメモリ上にバッファリングされる。

4. 定期的に、バッファリングされたペアは、パーティショニング機能によりR領域に分割され、ローカルディスクに書き込まれる。ローカルディスク上のバッファリングされたペアの位置はマスターに返され、マスターはこれらの位置をreduceワーカーに転送する責任を負う。

5. reduceワーカーは、マスターからこれらの位置が通知されると、リモートプロシージャ呼び出しを使用して、mapワーカーのローカルディスクからバッファリングデータを読み込みます。reduce workerはすべての中間データを読み込むと、中間キーでソートし、同じキーの出現はすべて一緒にされます。通常、多くの異なるキーが同じReduceタスクにマッピングされるため、ソートが必要です。中間データの量が多すぎてメモリに収まらない場合は、外部ソートを使用する。

6. reduce workerはソートされた中間データを繰り返し処理し、ユニークな中間キーに出会うたびに、そのキーと対応する中間値のセットをユーザーのReduce関数に渡します。Reduce関数の出力は、このReduceパーティションの最終出力ファイルに追加されます。

7. この時点で、ユーザープログラム内の MapReduce 呼び出しはユーザーコードに戻る。

通常、ユーザーはこれらのR出力ファイルを1つのファイルにまとめる必要はありません。これらのファイルを別のMapReduce呼び出しの入力として渡すか、複数のファイルに分割された入力を処理できる別の分散アプリケーションから使用することが多いのです。

<p align="center">
<img width="455" alt="スクリーンショット 2022-08-21 16 15 05" src="https://user-images.githubusercontent.com/55653825/185780086-f999dceb-b1b5-4efd-a194-663a4ceae05f.png">
</p>  


## 3.2 Master Data Structures
マスターはいくつかのデータ構造を保持します。各マップタスクとリダクションタスクについて、状態（アイドル、進行中、完了）、およびワーカーマシン（非アイドルタスクの場合）のIDを格納します。マスターは、中間ファイル領域の位置がマップタスクからリダクションタスクに伝搬されるパイプ役となります。そのため、マスターはマップタスクが生成したR個の中間ファイル領域の位置とサイズを保存しています。この位置とサイズの情報は、マップタスクが完了すると更新されます。この情報は、進行中のReduceタスクを持つワーカーにインクリメンタルにプッシュされます。


## 3.3 Fault Tolerance
MapReduce ライブラリは、数百、数千のマシンを使って非常に大量のデータを処理するために設計されているので、ライブラリはマシンの故障をうまく許容しなければなりません。

#### ワーカーの障害
マスターは定期的に各ワーカーに ping を送ります。ある時間内にワーカーからの応答がない場合、マスターはそのワーカーを失敗とマークします。ワーカーによって完了されたすべてのマップタスクは、最初のアイドル状態にリセットされ、他のワーカーのスケジューリングができるようになります。同様に、失敗したワーカー上で進行中のマップタスクや削減タスクもアイドルにリセットされ、再スケジューリングの対象となります。完了したマップタスクは、その出力が失敗したマシンのローカルディスクに保存されアクセスできないため、失敗時に再実行されます。マップタスクがワーカーAによって最初に実行され、その後ワーカーBによって実行された場合（Aが失敗したため）、リデュースタスクを実行するすべてのワーカーに再実行が通知されます。MapReduceは大規模なワーカー障害に強い。例えば、あるMapReduce処理中に、実行中のクラスタのネットワークメンテナンスのために、一度に80台のマシンのグループが数分間アクセス不能になったことがある。MapReduceマスターは、アクセス不能になったワーカーマシンの作業を再実行し、前進を続け、最終的にMapReduce処理を完了させました。

#### マスターの失敗
マスターに、上記のマスターデータ構造の定期的なチェックポイントを書かせるのは簡単です。マスタータスクが死亡した場合、最後にチェックポイントした状態から新しいコピーを開始することができます。しかし、マスターが1つしかないことを考えると、マスターが故障する可能性は低いです。したがって、現在の実装では、マスターが故障するとMapReduceの計算が中断されます。クライアントはこの状態を確認し、必要であればMapReduce処理を再試行することができます。

#### 障害が発生した場合のセマンティクス
ユーザが提供する map および reduce 演算子が入力値の決定論的関数であるとき、我々の分散実装は、プログラム全体の非故障逐次実行によって生成されたであろうものと同じ出力を生成する。この特性を達成するために、我々は map および reduce タスク出力の原子コミットに頼っている。進行中の各タスクは、その出力をプライベートな一時ファイルに書き出す。reduceタスクはそのようなファイルを1つ作成し、mapタスクはそのようなファイルをR個作成する（reduceタスクごとに1つ）。Mapタスクが完了すると、Workerはマスターにメッセージを送り、そのメッセージにR個のテンポラリファイルの名前を含めます。マスタは、すでに終了したマップタスクの完了メッセージを受け取った場合、そのメッセージを無視します。reduceタスクが完了すると、reduce workerは一時出力ファイルの名前を最終出力ファイルに変更する。同じreduceタスクが複数のマシンで実行される場合、同じ最終出力ファイルに対して複数のrenameコールが実行されることになります。この場合、セマンティクスが逐次実行と等価であるため、プログラマがプログラムの動作を推論することが非常に容易になります。map演算子やreduce演算子が非決定論的な場合、より弱いが合理的なセマンティクスを提供する。非決定論的演算子が存在する場合、特定のReduceタスクR1の出力は、非決定論的プログラムの逐次実行によって生成されるR1の出力と等価である。マップタスクMとリダクションタスクR1、R2を考え、コミットしたRithの実行をe(Ri)とする（このような実行はちょうど1つある）。e(R1)はMのある実行によって生成された出力を読み、e(R2)はMの異なる実行によって生成された出力を読むかもしれないので、弱い意味論が発生する。

## 3.4 ロカリティ
ネットワークバンド幅は、我々のコンピューティング環境では比較的希少な資源である。我々は、入力データ(GFS [8]によって管理)が我々のクラスタを構成するマシンのローカルディスクに格納されている事実を利用することによって、ネットワーク帯域幅を節約します。GFSは各ファイルを64MBのブロックに分割し、各ブロックの複数のコピー（通常3コピー）を異なるマシンに保存します。MapReduceマスタは、入力ファイルの位置情報を考慮し、対応する入力データのレプリカを含むマシン上でMaptaskのスケジューリングを試みます。そうでない場合は、そのタスクの入力データのレプリカの近くにマップタスクをスケジュールしようとします（たとえば、データを含むマシンと同じネットワークスイッチ上にあるワーカーマシン上で）。クラスタ内のかなりの割合のワーカーで大規模なMapReduce処理を実行する場合、ほとんどの入力データはローカルで読み込まれるため、ネットワーク帯域幅を消費しません。

## 3.5 タスクの粒度

上記のように、MapフェーズをM個に、ReduceフェーズをR個に細分化する。理想的には、MとRはワークマシンの数よりはるかに大きくなければなりません。各ワーカが多くの異なるタスクを実行することで、動的な負荷分散が向上し、また、ワーカーが故障したときの復旧が速くなります：ワーカーが完了した多くのMapタスクを他のすべてのワーカーマシンに分散することができます。  
マスターはO(M + R)個のスケジューリング決定を行い、O(M * R)個の状態をメモリに保持しなければならないので、我々の実装ではMとRの大きさに実用的な限界があります。(しかし、メモリ使用量の定数係数は小さく、O(M∗R)個の状態は、マップタスクとReduceタスクのペアごとに約1バイトのデータで構成されています)。  
さらに、各縮小タスクの出力は別々のファイルに出力されるため、Rはしばしばユーザによって制限されます。実際には、各タスクの入力データがおよそ16MBから64MBになるようにMを選択し（上記の局所性最適化が最も効果的になるように）、Rは使用する予定のワーカーマシン数の小さな倍数とする傾向があります。私たちはしばしば、M = 200,000、R = 5,000 で 2,000のワーカーマシンを使用してMapReduce計算を実行します。

## 3.6 バックアップタスク
MapReduce処理にかかる総時間を長くする一般的な原因の1つは、「はぐれ者」です。つまり、計算の最後のいくつかのmapまたはreduceタスクを完了するのに異常に長い時間を要するマシンです。はぐれ者は、多くの理由で発生する可能性があります。例えば、ディスクの不良により、修正可能なエラーが頻繁に発生し、読み取り性能が30MB/sから1MB/sに低下しているマシンがあります。クラスタスケジューリングシステムがそのマシンに他のタスクをスケジューリングし、CPU、メモリ、ローカルディスク、またはネットワーク帯域幅の競合により、MapReduceコードの実行が遅くなる可能性があります。私たちが最近経験した問題は、マシンの初期化コードのバグで、プロセッサのキャッシュが無効になったことです。MapReduce処理が完了に近づいたとき、マスターは進行中の残りのタスクのバックアップ実行をスケジュールします。プライマリ実行とバックアップ実行のいずれかが完了すると、タスクは完了とマークされます。このメカニズムは、通常、処理で使用される計算資源を数パーセント以下しか増加させないように調整されています。この結果、大規模なMapReduce処理を完了するまでの時間を大幅に短縮できることがわかりました。例として、セクション5.3で説明したソートプログラムは、バックアップタスクメカニズムを無効にすると、完了までに44%長い時間を要します。

# 4 リファインメント
Map関数とReduce関数を単純に記述することで提供される基本的な機能は、ほとんどのニーズに対して十分であるが、我々は、いくつかの拡張が有用であることを発見した。このセクションでは、これらの拡張について説明する。

## 4.1 パーティショニング機能
MapReduceのユーザーは、希望するReduceタスク/出力ファイルの数を指定する(R)。データは中間鍵のパーティショニング関数を使用して、これらのタスクに分割されます。デフォルトのパーティショニング関数は、ハッシュを使用したものです。しかし、場合によっては、キーの他の関数によってデータを分割することが有用である。たとえば、出力キーがURLで、1つのホストに対するすべてのエントリーが同じ出力ファイルになるようにしたい場合がある。このような状況をサポートするために、MapReduceライブラリのユーザは特別なパーティショニング関数を提供することができる。例えば、パーティショニング関数として "hash(Hostname(urlkey)) mod R" を使用すると、同じホストからのすべての URL が同じ出力ファイルに出力されることになる。

## 4.2 順序の保証
あるパーティションにおいて、中間的なキー/値ペアはキーの昇順で処理されることを保証する。この順序保証により、パーティションごとにソートされた出力ファイルを簡単に生成できる。これは、出力ファイル形式がキーによる効率的なランダムアクセス検索をサポートする必要がある場合や、出力データの利用者がデータをソートすることに便利である場合に有用である。

## 4.3 コンバイナー機能
場合によっては、各マップタスクが生成する中間キーに大きな繰り返しがあり、ユーザー指定の Reduce関数が可換・連想関数であることがある。この良い例が、セクション2.1の単語カウントの例である。単語の頻度はZipf分布に従う傾向があるので、各マップタスクは<the, 1>の形の数百から数千のレコードを生成することになります。これらのカウントはすべてネットワーク経由で1つのReduceタスクに送られ、Reduce関数で加算されて1つの数値が生成されます。Combiner関数はマップタスクを実行する各マシン上で実行されます。通常、コンバイナー関数とリデュース関数の両方を実装するために同じコードが使用されます。reduce関数とcombiner関数の唯一の違いは、MapReduceライブラリが関数の出力を処理する方法です。reduce関数の出力は最終出力ファイルに書き込まれます。部分結合は、特定のクラスの MapReduce 処理を大幅に高速化します。付録 A には、コンバイナーを使用する例が含まれています。

## 4.4 入力と出力のタイプ
MapReduceライブラリは、いくつかの異なるフォーマットで入力データを読み込むためのサポートを提供します。例えば、"text "モードの入力は各行をキーと値のペアとして扱う。キーはファイル内のオフセットで、値はその行のコンテンツである。もうひとつの一般的な形式は、キーでソートされたキーと値のペアの列を保存するものである。各入力型の実装は、それ自体を意味のある範囲に分割し、別のマップタスクとして処理する方法を知っている（例えば、テキストモードの範囲分割は、行の境界でのみ行われることを保証する）。ユーザは、簡単なリーダインタフェースの実装を提供することで、新しい入力タイプのサポートを追加することができるが、ほとんどのユーザは、少数の定義済み入力タイプのうちの1つを使用するだけである。同様に、異なる形式のデータを生成するための一連の出力型をサポートし、ユーザが新しい出力型のサポートを追加することも簡単です。

## 4.5 副作用
MapReduceのユーザーは、mapやreduce演算子からの追加出力として補助ファイルを生成することが便利であることを発見した場合がある。我々は、そのような副作用をアトミックかつべき等なものにするために、アプリケーション・ライターに依存している。したがって、ファイル間の一貫性が要求される複数の出力ファイルを生成するタスクは、決定論的であるべきです。この制限は、実際に問題となったことはない。

## 4.6 不良レコードのスキップ
ユーザーコードの中には、特定のレコードでMapReduce関数が決定論的にクラッシュする原因となるバグが存在することがあります。そのようなバグは、MapReduce 操作の完了を妨げます。通常の対処法はバグを修正することですが、これが実行不可能な場合もあります。バグがサードパーティのライブラリにあり、そのソースコードが入手できない場合もあります。また、大規模なデータの統計解析を行う場合など、少数のレコードを無視することが許容される場合があります。MapReduceライブラリは、決定論的なクラッシュを引き起こすレコードを検出し、前進するためにこれらのレコードをスキップするオプションの実行モードを提供します。各ワーカープロセスは、セグメント違反とバスエラーをキャッチするシグナルハンドラをインストールします。MapReduceライブラリは、ユーザーのMapまたはReduce操作を呼び出す前に、引数のシーケンス番号をグローバル変数に格納します。ユーザーコードがシグナルを生成すると、シグナルハンドラはシーケンス番号を含む "最後のあがき "UDPパケットをMapReduceマスターに送ります。マスターは、特定のレコードで複数の失敗を見た場合、対応するMapまたはReduceタスクの次の再実行を発行するときに、そのレコードをスキップする必要があることを示す。

## 4.7 ローカル実行
実際の計算は分散システムで行われ、多くの場合数千台のマシンで行われ、マスターによって動的に作業割り当てが決定されるため、MapまたはReduce関数の問題のデバッグは困難な場合があります。デバッグ、プロファイリング、小規模テストを容易にするために、我々は、ローカルマシン上でMapReduce操作のすべての作業を順次実行するMapReduceライブラリの代替実装を開発した。MapReduce処理に必要なすべての作業をローカルマシンで逐次実行する実装を開発した。ユーザは、計算を特定のMaptaskに限定することができるよう制御できる。ユーザは、特別なプログラムを起動し、有用と思われるデバッグまたはテストツール (例: gdb) を容易に使用することができます。

## 4.8 ステータス情報
マスターは内部でHTTPサーバーを実行し、人間が消費するための一連のステータス・ページをエクスポートします。ステータスページには、完了したタスクの数、進行中のタスクの数、入力バイト数、中間データバイト数、出力バイト数、処理速度など、計算の進行状況が表示される。また、各タスクが生成する標準エラーファイルや標準出力ファイルへのリンクも含まれている。さらに、トップレベルのステータスページには、どのワーカーが失敗したか、また失敗時にどのマップおよびリデュースタスクを処理していたかが表示されます。この情報は、ユーザーコードのバグを診断する際に有用です。

## 4.9 カウンター
MapReduceライブラリは、様々なイベントの発生をカウントするためのカウンタ機能を提供します。
様々なイベントの発生をカウントするためのカウンタ機能を提供します。例えば、ユーザー
コードでは、処理された単語の総数を数えたいかもしれません。
あるいは、インデックスを作成したドイツ語ドキュメントの数などです。
この機能を使うには、ユーザーコードは名前付きカウンター
オブジェクトを作成し、Map関数やReduce関数内でそのカウンターを適切にインクリメントします。
Map関数やReduce関数の中で適切にカウンターを増分します。例えば 

```
Counter* uppercase;
uppercase = GetCounter("uppercase");

map(String name, String contents):
    for each word w in contents:
        if (IsCapitalized(w)):
            uppercase->Increment();
        EmitIntermediate(w, "1");
```

個々のワーカーマシンからのカウンタ値
は定期的にマスターに伝搬されます（Pingレスポンスにピギーバックされます）。
例：個々のワーカーマシンからのカウンタ値が定期的にマスターに伝えられる（Pingレスポンスに便乗）。マスターは、成功したMapおよびReduceタスクのカウンタ値を集約し
を集計し、MapReduce処理時にユーザーコードに返します。
MapReduce処理が完了すると、カウンタ値をユーザーコードに返します。
MapReduce処理が完了した時点でユーザーコードに返します。現在のカウンタ値はマスターのステータスページにも表示されるので、人間はライブで計算の進行状況を見ることができます。
現在のカウンタ値はマスターのステータスページにも表示され、人間がライブで計算の進行状況を確認できるようになっています。カウンタ値を集計する際、マスターは以下の影響を排除します。
同じMapタスクやReduceタスクの重複実行の影響を排除し、二重計算を回避します。
二重計算を避けるためです。(重複実行は
バックアップタスクの使用や、障害によるタスクの再実行により、重複実行が発生する可能性があります)。
重複実行はバックアップタスクの使用や、障害によるタスクの再実行によって発生します)。
いくつかのカウンタ値は、MapReduceライブラリによって自動的に維持されます。
MapReduceライブラリによって自動的に維持されるカウンタ値もあります。
の数などです。
ユーザーは、MapReduce操作の動作の健全性をチェックするために、カウンタ機能が有用であることを発見しました。たとえば
たとえば、あるMapReduce処理では、ユーザーコード
あるMapReduce処理では、ユーザーコードは、生成された出力ペアの数が入力ペアの数と正確に等しいことを
あるいは、処理されたドイツ語のドキュメントの割合が、処理されたドキュメントの総数のうち許容範囲内の割合であることを確認したいかもしれません。
